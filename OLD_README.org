
The implementer's job is to provide both the worker tasks (~tasks.py~) and the
driver code suitable for performing the scoring. Depending on the scoring
algorithm(s), there are multiple considerations that may be addressed, such as:

 - running a heterogeneous workload, e.g. with multiple task types
 - gracefully handling failures, e.g. enabling partial results
 - fine-grained control over task execution time limits
 - tuning and profiling

Given the flexibility of Celery and Python, there are multiple valid ways to
achieve the above goals. We provide two examples to illustrate possible ways
to set up the job queue, and briefly discuss alternatives in the module and
function documentation.

 - ~scorer_worker/scorer_basic.py~
 - ~scorer_worker/scorer_advanced.py~

Advanced example in particular makes an attempt to address the above-mentioned
complexities.

* Running tests

We provide a makefile to run tests. You can run the tests using ~make test~ in
repo root.

This will spin up Redis + db containers, run the tests, and tear down the containers.
Running this command is a good way to ensure that your environment is set up correctly.

Be aware that due to the way pytest interacts with Celery, you need to ensure
that no other Celery workers are running when you run the tests.

Note also that test code runs outside docker, therefore Redis and Postgres are
made available in the localhost test environment via port mappings in the docker
compose file.

Test rely on sample data being present in a test database (~posts_test_db~).
Note that this is a /different/ database from the one discuss in the /sample
data/ section above, and contains a small subset of test records.

We provide a script that seeds both the test database and the sample posts
database (with a simulated user pool). You are welcome to adjust the sample post
database seeding process as best fits your needs. This script is available as
~ci.sh~, and is best executed as ~make ci~ in repo root, as it relies on the db container.

Simplified script is reproduced below for your reference:

#+begin_src bash
# Postgres connection setup. This will depend on your specific port mappings
# and other settings.
PGHOST=localhost
PGPORT=5435

PGDB_PYTEST_TESTS=posts_test_db
PGDB_SAMPLE_DATA=posts

POSTS_DB_URI_PYTEST_TESTS=postgresql://postgres:postgres@${PGHOST}:${PGPORT}/${PGDB_PYTEST_TESTS}
export POSTS_DB_URI=postgresql://postgres:postgres@${PGHOST}:${PGPORT}/${PGDB_SAMPLE_DATA}

cd "${PROJECT_ROOT}/sample_data" || exit 1

POSTS_DB_URI=${POSTS_DB_URI_PYTEST_TESTS} poetry run python seed_post_db.py --dbname ${TEST_POSTS_DB} --seed-postgres

# Setup sample posts database
# This simulates a pool of 500 users with varying activity levels.

poetry run python seed_post_db.py --n-users=500 --activity-distribution=0.2:20,1:65,5:15
#+end_src